<p style="text-align: justify;"><strong>PREVENT: Proactive Risk Evaluation and Vigilant Execution of Navigation and Manipulation Tasks for Mobile Robotic Chemists</strong></p>

<!-- <div style="text-align: center;">
<em><strong>Satheeshkumar Veeramani</strong><sup>1</sup>, Zhengxue Zhou<sup>1</sup>, Francisco Munguia-Galeano<sup>1</sup>, Hatem Fakhruldeen<sup>1</sup>, Thomas Roddelkopf<sup>2</sup>, Mohammed Faeik Ruzaij Al-Okby<sup>2</sup>, Kerstin Thurow<sup>2</sup>, Andrew Ian Cooper<sup>1,*</sup>.</em> 
</div>

<div style="text-align: center;">
  <p><em> <sup>1</sup>Department of Chemistry and Materials Innovation Factory, University of Liverpool, Liverpool, United Kingdom; <sup>2</sup>Center for Life Science Automation (CELISCA), University of Rostock, Rostock, Germany</em></p>
</div> -->


<p style="text-align: justify;">Mobile robotic chemists are a fast growing trend in the field of chemistry and materials research. However, so far these mobile robots lack workflow awareness skills. This poses the risk that even a small anomaly, such as an improperly capped sample vial could disrupt the entire workflow. This wastes time, and resources, and could pose risks to human researchers, such as exposure to toxic materials. Unimodal perception mechanisms can be used to predict anomalies but they often generate excessive false positives. This may halt workflow execution unnecessarily, requiring researchers to intervene and to resume the workflow when no problem actually exists, negating the benefits of autonomous operation. To address this problem, we propose navigation and manipulation skills based on a multimodal Behavior Tree (BT) approach that can be integrated into existing software architectures with minimal modifications. Our approach involves a hierarchical perception mechanism that exploits AI techniques (CNNs and VLMs) and sensory feedback through Dexterous Vision and Navigational Vision cameras and an IoT gas sensor module for execution-related decision-making. Experimental evaluations show that the proposed approach is comparatively efficient and completely avoids both false negatives and false positives when tested in simulated risk scenarios within our robotic chemistry workflow. The results also show that the proposed multi-modal perception skills achieved deployment accuracies that were higher than the average of the corresponding uni-modal skills, both for navigation and for manipulation. </p>

<!-- <div style="text-align: center; margin-top: 2em;">
  <h3>Graphical Abstract</h3>
  <img src="/images/PREVENT/SA.png" alt="" style="max-width: 100%; height: auto;">
</div>

<div style="text-align: center; margin-top: 2em;">
  <h3>Figure: Coordinated Inspection and Navigation (CIN) Skill</h3>
  <img src="/images/PREVENT/CIN.png" alt="Behavior Tree for Safe Navigation" style="max-width: 100%; height: auto;">
</div>



<div style="text-align: center; margin-top: 2em;">
  <h3>Figure: Inspection Before Manipulation (IBM) Skill</h3>
  <img src="/images/PREVENT/IBM.png" alt="Behavior Tree for Safe Navigation" style="max-width: 100%; height: auto;">
</div> -->




ðŸ“Š [Click here](https://theuniversityofliverpool-my.sharepoint.com/:f:/r/personal/sathiz52_liverpool_ac_uk/Documents/Liverpool/Satheesh%20paper/Dataset?csf=1&web=1&e=1zrwMO) to access the dataset

<!-- repo link
<div style="text-align: center; margin-top: 20px;">
  <a href="https://theuniversityofliverpool-my.sharepoint.com/:f:/r/personal/sathiz52_liverpool_ac_uk/Documents/Liverpool/Satheesh%20paper/Dataset?csf=1&web=1&e=1zrwMO" target="_blank" style="text-decoration: none; color: #333;">
    <i class="fab fa-github" style="font-size: 1.8em; vertical-align: middle;"></i>
    <span style="margin-left: 8px; font-size: 1.2em;">Dataset</span>
  </a>
</div> -->


<!-- https://hits.sh/satheezv.github.io.svg?style=plastic&label=Page%20Visits -->